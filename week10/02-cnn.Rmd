---
title: "A simple convolutional neural network in Keras"
date: "22 March 2023"
output: html_document
---

Code by RStudio. Their website recently good updated, but see the references for closest current sources.

In this notebook, we will train a very simple CNN and show that it already performs reasonably well on a task which is much harder than the MNIST classification.

Loading packages:

```{r}
library("tensorflow")
library("keras")
```

Downloading the cifar data (this downloads roughly 700 MB of data):

```{r}
cifar <- dataset_cifar10()

# Note that training and test set can be accessed as:

# cifar$train$x
# cifar$train$y

# cifar$test$x
# cifar$test$y

```

Each observation has 32x32 pixels and three colour channels (R,G,B):

```{r}
dim(cifar$train$x)
```

Visualising the data:

```{r}
class_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck')

index <- 1:30

par(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4))
cifar$train$x[index,,,] %>% 
  purrr::array_tree(1) %>%
  purrr::set_names(class_names[cifar$train$y[index] + 1]) %>% 
  purrr::map(as.raster, max = 255) %>%
  purrr::iwalk(~{plot(.x); title(.y)})
```

Define and compile the model:

```{r}
model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", 
                input_shape = c(32,32,3)) %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

summary(model)

model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)
```

Training:

```{r}
history <- model %>% 
  fit(
    x = cifar$train$x, y = cifar$train$y,
    epochs = 10,
    validation_data = unname(cifar$test),
    verbose = 2
  )

```

Evaluation:

Plot:

```{r}
plot(history)
```

Output metrics:

```{r}
evaluate(model, cifar$test$x, cifar$test$y, verbose = 0)
```

Also here you can train the model for longer, or try out different architectures and hyper-parameters to see whether you can increase the accuracy.


References

  - Original source (inactive at the moment): https://tensorflow.rstudio.com/tutorials/advanced/images/cnn/
  - Similar current RStudio materials: https://tensorflow.rstudio.com/examples/cifar10_cnn and  https://github.com/rstudio/keras/blob/main/vignettes/examples/cifar10_cnn.R