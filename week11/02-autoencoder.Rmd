---
title: "Nonlinear dimension reduction with autoencoders"
author: "Friedrich Geiecke"
date: "29 March 2023"
output: html_document
---

--------------------------------------------------------------------------------

#### Preliminary remarks

If you have installed `keras` already, this notebook should run. Otherwise you can install `keras` with:

1. Run `install.packages("keras")`
2. Load the package with `library("keras")`
3. Then run the function `install_keras()` in the R-console

Afterwards the code in this notebook should run properly.

For more information on `tensorflow` (the basis of `keras`) see: https://tensorflow.rstudio.com/install/index.html

For issues resulting from already existing Python installations, the following can be helpful: https://tensorflow.rstudio.com/install/custom

--------------------------------------------------------------------------------

This file is an outlook on non-linear dimension reduction. Already basic neural network architectures can also be used for this task as described in the [paper](https://www.cs.toronto.edu/~hinton/science.pdf) "Reducing the Dimensionality of Data with Neural Networks" by Hinton and Salakhutdinov (Science, 2006) which was followed by much research on so called "autoencoders". In the following, I have translated the code of a post [here](https://stats.stackexchange.com/questions/190148/building-an-autoencoder-in-tensorflow-to-surpass-pca), which tries to create similar outcomes to the ones in the paper with a somewhat newer and simpler architecture, into R.


```{r}
library("keras")
library("tensorflow")
library("ggplot2")
```

```{r}
set_random_seed(100)
```

Loading the MNIST dataset:

```{r}
# Training and test data (already shuffled)
c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()

# Reshaping 28x28 images to 1x784
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))

# Divide by the max value to transform into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255

cat(nrow(x_train), 'train observations\n')
cat(nrow(x_test), 'test observations\n')
```


### Principal component analysis (PCA)

Running the PCA:

```{r}
pca_output <- prcomp(x_train, scale = FALSE)
# For faster computation, use e.g. only the first 5000 observations for the PCA
#pca_output <- prcomp(x_train[1:5000,], scale = FALSE)
```

Storing the first two principal components and reconstructing the original data with them:

```{r}
# First two principal components
Z_pca <-pca_output$x[,1:2]
# Reconstruction of original data from the 2-dimensional PCA representation
R_pca <- t(t(Z_pca %*% t(pca_output$rotation[,1:2])) + pca_output$center)
```


### Autoencoder

Building the neural network architecture:

```{r}
autoencoder <- keras_model_sequential()
autoencoder %>%
  layer_dense(units = 512, activation = "elu", input_shape = c(784)) %>%  
  layer_dense(units = 128, activation = "elu") %>%
  layer_dense(units = 2, activation = "linear", name = "bottleneck") %>%
  layer_dense(units = 128, activation = "elu") %>%
  layer_dense(units = 512, activation = "elu") %>%
  layer_dense(units = 784, activation = "sigmoid") # sigmoid output because
# true output is also normalised on [0,1]

summary(autoencoder)

autoencoder %>% compile(
  loss = "mean_squared_error",
  optimizer = optimizer_adam()
)
```

Note how this cleverly transforms a dimension reduction tasks into a supervised learning problem, trying to predict/recreate the original data after having passed it through a bottleneck layer with only two neurons. Note also that already this neural network has almost 1 million parameters which are trained.

Training:

```{r}
history <- autoencoder %>% fit(
  x_train, x_train,
  batch_size = 128,
  epochs = 8,
  verbose = 1,
  validation_data = list(x_test, x_test)
)
```

Bottleneck representation values and reconstruction of input data:

```{r}
# Extracting the encoder part from the model
encoder <- keras_model(autoencoder$input, get_layer(autoencoder, name = "bottleneck")$output)

Z_ae <- encoder %>% predict(x_train)
R_ae <- autoencoder %>% predict(x_train)
```


### Visuallsation: Low dimensional representations

```{r}
# Only first 5000 observations are depicted in order to reduce overplotting
plot_data = data.frame(z1_pca = Z_pca[1:5000, 1], z2_pca = Z_pca[1:5000, 2],
                       z1_ae = Z_ae[1:5000, 1], z2_ae = Z_ae[1:5000, 2],
                       Digit = factor(y_train[1:5000] + 1))
```

```{r}
ggplot(plot_data, aes(x = z1_pca, y = z2_pca, color = Digit)) + 
    geom_point(size = 1) + ggtitle("PCA") +
  theme(plot.title = element_text(hjust = 0.5)) + xlab("z1") + ylab("z2")
ggplot(plot_data, aes(x = z1_ae, y = z2_ae, color = Digit)) +
    geom_point(size = 1) + ggtitle("Autoencoder") + 
  theme(plot.title = element_text(hjust = 0.5)) + xlab("z1") + ylab("z2")
```

### Visualisation: Reconstruction of original data

```{r}
indices <- 1:30 # looking at the first 30 observations
x_train_plot <- array_reshape(x_train[indices,], c(max(indices), 28, 28))
x_train_ae_plot <- array_reshape(R_ae[indices,], c(max(indices), 28, 28))
x_train_pca_plot <- array_reshape(R_pca[indices,], c(max(indices), 28, 28))
x_train_pca_plot[x_train_pca_plot < 0] <- 0 # some of the PCA reconstructions
# can have values smaller than 0 or larger than 1
x_train_pca_plot[x_train_pca_plot > 1] <- 1
y_train_plot <- y_train[indices]

plot_mnist <- function(x, y) {
  
  par(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4))
  x %>% 
  purrr::array_tree(1) %>%
  purrr::set_names(y) %>% 
  purrr::map(as.raster, max = 1) %>%
  purrr::iwalk(~{plot(.x); title(.y)})
  
}
```

```{r}
plot_mnist(x_train_plot, y_train_plot)
plot_mnist(x_train_pca_plot, y_train_plot)
plot_mnist(x_train_ae_plot, y_train_plot)
```

Considering that the whole information was stored in only two floating point numbers for each image, the reconstruction of this simple autoencoder is already quite remarkable. Reducing the data to two dimensions makes it possible to plot the lower dimensional representation in a plane, however, if you reduce the data to e.g. 10 dimensions with the autoencoder, then the reconstructed images will already be quite close to the original ones.

Different modifications of autoencoders can e.g. be used for standard dimension reduction, denoising of images, or generative modelling. If you are interested in the topic and would like to study it further after the next week on neural networks, see online discussions like [this](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798) one for a start and for a more rigorous discussion see the deep learning [textbook](https://www.deeplearningbook.org/contents/autoencoders.html) chapter on autoeoncoders. More advanced current autoencoders are e.g. "variational autoencoders" (see e.g. this [video](https://youtu.be/9zKuYvjFFS8) for an easy to watch introduction).


### References

- https://stats.stackexchange.com/questions/190148/building-an-autoencoder-in-tensorflow-to-surpass-pca
- https://www.cs.toronto.edu/~hinton/science.pdf
- https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_mlp.R
- https://tensorflow.rstudio.com/tutorials/advanced/images/cnn/
- https://www.deeplearningbook.org/contents/autoencoders.html
